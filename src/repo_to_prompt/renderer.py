"""Output rendering module for repo-to-prompt.

Generates Markdown context packs, JSONL chunks, and reports.
All output is deterministic when timestamps are disabled.
"""

from __future__ import annotations

import contextlib
import json
from datetime import datetime, timezone
from pathlib import Path

from .config import Chunk, FileInfo, OutputMode, ScanStats
from .ranker import FileRanker
from .scanner import generate_tree
from .utils import read_file_safe


class ContextPackRenderer:
    """
    Renders a Markdown context pack for LLM prompting.

    Structure:
    1. REPO OVERVIEW - Summary, purpose, key info
    2. DIRECTORY TREE - Visual structure
    3. KEY FILES - READMEs, configs, entrypoints
    4. CODE MAP - Per-language module listing
    5. FILE CONTENTS - Chunked content with citations
    """

    def __init__(
        self,
        root_path: Path,
        files: list[FileInfo],
        chunks: list[Chunk],
        ranker: FileRanker,
        stats: ScanStats,
        max_total_tokens: int | None = None,
        include_timestamp: bool = True,
    ):
        """
        Initialize the renderer.

        Args:
            root_path: Repository root path
            files: List of scanned files (should be pre-sorted for determinism)
            chunks: List of content chunks (should be pre-sorted for determinism)
            ranker: File ranker with manifest info
            stats: Scan statistics
            max_total_tokens: Maximum total tokens in output
            include_timestamp: Whether to include timestamps (disable for reproducible output)
        """
        self.root_path = root_path
        self.files = files
        self.chunks = chunks
        self.ranker = ranker
        self.stats = stats
        self.max_total_tokens = max_total_tokens
        self.include_timestamp = include_timestamp

        # Organize files by type
        self._readme_files = [f for f in files if f.is_readme]
        self._config_files = [f for f in files if f.is_config and not f.is_readme]
        self._doc_files = [f for f in files if f.is_doc and not f.is_readme]

    def render(self) -> str:
        """Render the complete context pack."""
        sections = []

        # Header
        sections.append(self._render_header())

        # Overview
        sections.append(self._render_overview())

        # Directory tree
        sections.append(self._render_tree())

        # Key files summary
        sections.append(self._render_key_files())

        # Code map
        sections.append(self._render_code_map())

        # File contents
        sections.append(self._render_contents())

        return "\n\n".join(filter(None, sections))

    def _render_header(self) -> str:
        """Render the document header."""
        repo_name = self.root_path.name

        if self.include_timestamp:
            timestamp_line = f"\n> Generated by repo-to-prompt on {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}"
        else:
            timestamp_line = "\n> Generated by repo-to-prompt"

        return f"""# Repository Context Pack: {repo_name}
{timestamp_line}
> Files: {self.stats.files_included} | Chunks: {self.stats.chunks_created} | Size: {self.stats.total_bytes_included:,} bytes

---"""

    def _render_overview(self) -> str:
        """Render the repository overview section."""
        lines = ["## ðŸ“‹ Repository Overview"]

        # Try to extract README content for summary
        readme_content = ""
        if self._readme_files:
            with contextlib.suppress(Exception):
                readme_content, _ = read_file_safe(self._readme_files[0].path, max_bytes=4000)

        # Project info from manifests
        manifest_info = self.ranker.get_manifest_info()

        if manifest_info.get("name"):
            lines.append(f"\n**Project:** {manifest_info['name']}")

        if manifest_info.get("description"):
            lines.append(f"\n**Description:** {manifest_info['description']}")

        # Detected languages
        self.ranker.get_detected_languages()
        lang_stats = self.stats.languages_detected

        if lang_stats:
            lang_summary = ", ".join(
                f"{lang} ({count})"
                for lang, count in sorted(lang_stats.items(), key=lambda x: -x[1])[:5]
            )
            lines.append(f"\n**Languages:** {lang_summary}")

        # Entrypoints
        entrypoints = self.ranker.get_entrypoints()
        if entrypoints:
            lines.append("\n**Entrypoints:**")
            for ep in sorted(entrypoints)[:10]:
                lines.append(f"- `{ep}`")

        # Build/test commands from package.json scripts
        scripts = manifest_info.get("scripts", {})
        if scripts:
            lines.append("\n**Available Commands:**")
            for cmd in ["build", "test", "start", "dev", "lint"]:
                if cmd in scripts:
                    lines.append(
                        f"- `{cmd}`: `{scripts[cmd][:60]}...`"
                        if len(scripts[cmd]) > 60
                        else f"- `{cmd}`: `{scripts[cmd]}`"
                    )

        # README excerpt
        if readme_content:
            # Get first meaningful section
            readme_lines = readme_content.split("\n")
            excerpt_lines = []
            in_content = False

            for line in readme_lines[:50]:
                # Skip badges and initial headers
                if line.strip() and not line.startswith("![") and not line.startswith("[!["):
                    in_content = True
                if in_content:
                    excerpt_lines.append(line)
                    if len(excerpt_lines) >= 15:
                        break

            if excerpt_lines:
                excerpt = "\n".join(excerpt_lines)
                lines.append(f"\n**README Excerpt:**\n\n{excerpt}")
                if len(readme_lines) > 50:
                    lines.append("\n*[README truncated...]*")

        return "\n".join(lines)

    def _render_tree(self) -> str:
        """Render the directory tree section."""
        # Highlight important files
        important_files = {f.relative_path for f in self.files if f.priority >= 0.8}

        tree = generate_tree(
            self.root_path,
            max_depth=4,
            include_files=True,
            files_to_highlight=important_files,
        )

        return f"""## ðŸ“ Directory Structure

```
{tree}
```

*â­ = Important file*"""

    def _render_key_files(self) -> str:
        """Render the key files summary section."""
        lines = ["## ðŸ”‘ Key Files"]

        # Group files by category
        categories = [
            ("Documentation", [f for f in self.files if f.is_doc][:5]),
            ("Configuration", [f for f in self.files if f.is_config][:5]),
            ("Entrypoints", [f for f in self.files if "entrypoint" in f.tags][:5]),
        ]

        for category, category_files in categories:
            if category_files:
                lines.append(f"\n### {category}")
                for f in category_files:
                    priority_str = f"({f.priority:.0%})" if f.priority else ""
                    lines.append(f"- `{f.relative_path}` {priority_str}")

        return "\n".join(lines)

    def _render_code_map(self) -> str:
        """Render the code map section (per-language module listing)."""
        lines = ["## ðŸ—ºï¸ Code Map"]

        # Group files by language
        files_by_lang: dict[str, list[FileInfo]] = {}
        for f in self.files:
            if f.language not in files_by_lang:
                files_by_lang[f.language] = []
            files_by_lang[f.language].append(f)

        # Sort languages by file count
        sorted_langs = sorted(files_by_lang.items(), key=lambda x: -len(x[1]))

        for lang, lang_files in sorted_langs[:5]:
            if lang in ("text", "markdown"):
                continue

            lines.append(f"\n### {lang.title()} ({len(lang_files)} files)")

            # Group by directory
            dirs: dict[str, list[FileInfo]] = {}
            for f in lang_files:
                dir_path = str(Path(f.relative_path).parent)
                if dir_path == ".":
                    dir_path = "(root)"
                if dir_path not in dirs:
                    dirs[dir_path] = []
                dirs[dir_path].append(f)

            for dir_path, dir_files in sorted(dirs.items())[:10]:
                lines.append(f"\n**{dir_path}/**")
                for f in sorted(dir_files, key=lambda x: x.path.name)[:10]:
                    lines.append(f"- `{f.path.name}`")
                if len(dir_files) > 10:
                    lines.append(f"- *...and {len(dir_files) - 10} more*")

        return "\n".join(lines)

    def _render_contents(self) -> str:
        """Render the file contents section with chunks."""
        lines = ["## ðŸ“„ File Contents"]

        # Group chunks by file
        chunks_by_file: dict[str, list[Chunk]] = {}
        for chunk in self.chunks:
            if chunk.path not in chunks_by_file:
                chunks_by_file[chunk.path] = []
            chunks_by_file[chunk.path].append(chunk)

        # Sort files by priority (from the original file list)
        file_priorities = {f.relative_path: f.priority for f in self.files}
        sorted_files = sorted(chunks_by_file.keys(), key=lambda p: (-file_priorities.get(p, 0), p))

        total_tokens = 0

        for file_path in sorted_files:
            file_chunks = chunks_by_file[file_path]

            # Sort chunks by line number
            file_chunks.sort(key=lambda c: c.start_line)

            # Get file info
            file_info = next((f for f in self.files if f.relative_path == file_path), None)

            lang = file_chunks[0].language if file_chunks else "text"
            priority = file_info.priority if file_info else 0.5

            lines.append(f"\n### `{file_path}`")
            lines.append(
                f"*Priority: {priority:.0%} | Language: {lang} | Chunks: {len(file_chunks)}*"
            )

            for chunk in file_chunks:
                # Check token budget
                if (
                    self.max_total_tokens
                    and total_tokens + chunk.token_estimate > self.max_total_tokens
                ):
                    lines.append("\n*[Content truncated due to token limit]*")
                    return "\n".join(lines)

                total_tokens += chunk.token_estimate

                # Citation header
                lines.append(f"\n**Lines {chunk.start_line}-{chunk.end_line}:**")

                # Code block
                lines.append(f"```{lang}")
                lines.append(chunk.content.rstrip())
                lines.append("```")

        return "\n".join(lines)


class JSONLRenderer:
    """Renders chunks as JSONL for RAG/embedding."""

    def __init__(self, chunks: list[Chunk]):
        """
        Initialize the renderer.

        Args:
            chunks: List of chunks to render
        """
        self.chunks = chunks

    def render(self) -> str:
        """Render chunks as JSONL string."""
        lines = []
        for chunk in self.chunks:
            lines.append(json.dumps(chunk.to_dict(), ensure_ascii=False))
        return "\n".join(lines)

    def write(self, output_path: Path) -> None:
        """Write chunks to JSONL file."""
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            for chunk in self.chunks:
                # Sort dict keys for deterministic output
                f.write(json.dumps(chunk.to_dict(), ensure_ascii=False, sort_keys=True))
                f.write("\n")


class ReportRenderer:
    """
    Renders the processing report as JSON.

    Report schema is versioned for backwards compatibility.
    """

    def __init__(
        self,
        stats: ScanStats,
        config: dict,
        output_files: list[str],
        include_timestamp: bool = True,
        files: list[FileInfo] | None = None,
    ):
        """
        Initialize the renderer.

        Args:
            stats: Scan statistics
            config: Configuration used
            output_files: List of generated output files
            include_timestamp: Whether to include generation timestamp
            files: List of files processed (for file manifest)
        """
        from .config import REPORT_SCHEMA_VERSION

        self.stats = stats
        self.config = config
        self.output_files = sorted(output_files)  # Sort for determinism
        self.include_timestamp = include_timestamp
        self.files = files or []
        self.schema_version = REPORT_SCHEMA_VERSION

    def render(self) -> dict:
        """Render the report as a dictionary."""
        report = {
            "schema_version": self.schema_version,
        }
        if self.include_timestamp:
            report["generated_at"] = datetime.now(timezone.utc).isoformat()

        report["stats"] = self.stats.to_dict()
        report["config"] = self.config
        report["output_files"] = self.output_files

        # Include file manifest with IDs for deterministic reference
        if self.files:
            report["files"] = [
                {
                    "id": f.id,
                    "path": f.relative_path,
                    "priority": round(f.priority, 3),
                    "tokens": f.token_estimate,
                }
                for f in sorted(self.files, key=lambda x: (-x.priority, x.relative_path))
            ]

        return report

    def write(self, output_path: Path) -> None:
        """Write report to JSON file."""
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(self.render(), f, indent=2, ensure_ascii=False, sort_keys=True)


def render_context_pack(
    root_path: Path,
    files: list[FileInfo],
    chunks: list[Chunk],
    ranker: FileRanker,
    stats: ScanStats,
    max_total_tokens: int | None = None,
    include_timestamp: bool = True,
) -> str:
    """
    Render a context pack as Markdown.

    Args:
        root_path: Repository root path
        files: List of scanned files (should be pre-sorted for determinism)
        chunks: List of content chunks (should be pre-sorted for determinism)
        ranker: File ranker with manifest info
        stats: Scan statistics
        max_total_tokens: Maximum total tokens
        include_timestamp: Whether to include generation timestamp

    Returns:
        Rendered Markdown string
    """
    renderer = ContextPackRenderer(
        root_path=root_path,
        files=files,
        chunks=chunks,
        ranker=ranker,
        stats=stats,
        max_total_tokens=max_total_tokens,
        include_timestamp=include_timestamp,
    )
    return renderer.render()


def render_jsonl(chunks: list[Chunk]) -> str:
    """
    Render chunks as JSONL string.

    Args:
        chunks: List of chunks (should be pre-sorted for determinism)

    Returns:
        JSONL string with one JSON object per line
    """
    renderer = JSONLRenderer(chunks)
    return renderer.render()


def write_outputs(
    output_dir: Path,
    mode: OutputMode,
    context_pack: str,
    chunks: list[Chunk],
    stats: ScanStats,
    config: dict,
    include_timestamp: bool = True,
    files: list[FileInfo] | None = None,
) -> list[str]:
    """
    Write all outputs to the output directory.

    Output is deterministic when include_timestamp=False: same input
    will produce byte-identical output files.

    Args:
        output_dir: Output directory path
        mode: Output mode (prompt, rag, or both)
        context_pack: Rendered context pack Markdown
        chunks: List of chunks (should be pre-sorted for determinism)
        stats: Scan statistics
        config: Configuration used (should have sorted keys)
        include_timestamp: Whether to include timestamps in report
        files: List of processed files (for file manifest in report)

    Returns:
        Sorted list of generated file paths
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    output_files = []

    # Write context pack
    if mode in (OutputMode.PROMPT, OutputMode.BOTH):
        context_path = output_dir / "context_pack.md"
        with open(context_path, "w", encoding="utf-8") as f:
            f.write(context_pack)
        output_files.append(str(context_path))

    # Write JSONL chunks
    if mode in (OutputMode.RAG, OutputMode.BOTH):
        jsonl_path = output_dir / "chunks.jsonl"
        jsonl_renderer = JSONLRenderer(chunks)
        jsonl_renderer.write(jsonl_path)
        output_files.append(str(jsonl_path))

    # Write report (after other files so we can include them)
    report_path = output_dir / "report.json"
    report_renderer = ReportRenderer(
        stats,
        config,
        output_files,
        include_timestamp=include_timestamp,
        files=files,
    )
    report_renderer.write(report_path)
    output_files.append(str(report_path))

    # Return sorted for determinism
    return sorted(output_files)
